{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7ff4ca1-8435-41cf-a743-7190695a7fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install required libraries\n",
    "%pip install openai python-dotenv PyPDF2\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d811a9d7-56bb-4884-a635-04a2fec359d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Azure OpenAI Configuration\n",
    "import os\n",
    "\n",
    "# Canada East - Embedding Model\n",
    "AZURE_OPENAI_KEY = \"\"\n",
    "AZURE_OPENAI_ENDPOINT = \"\"\n",
    "AZURE_OPENAI_API_VERSION = \"\"\n",
    "EMBEDDING_DEPLOYMENT = \"\"\n",
    "\n",
    "# East US 2 - GPT Model\n",
    "AZURE_OPENAI_KEY_EASTUS2 = \"\"\n",
    "AZURE_OPENAI_ENDPOINT_EASTUS2 = \"\"\n",
    "GPT_DEPLOYMENT = \"\"\n",
    "\n",
    "print(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bcff3be-d97d-4dfc-9568-764dc0c242a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Test Azure OpenAI from Databricks\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Initialize clients\n",
    "client_canada = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "client_eastus = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_KEY_EASTUS2,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT_EASTUS2\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = client_canada.embeddings.create(\n",
    "    model=EMBEDDING_DEPLOYMENT,\n",
    "    input=\"Test from Databricks\"\n",
    ")\n",
    "\n",
    "print(f\"Embedding works! Dimension: {len(test_embedding.data[0].embedding)}\")\n",
    "\n",
    "# Test GPT\n",
    "test_chat = client_eastus.chat.completions.create(\n",
    "    model=GPT_DEPLOYMENT,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello from Databricks!\"}],\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "print(f\"GPT works! Response: {test_chat.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b8f06dc-9abc-4e74-9ca9-e65bd2a75ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Create database structure\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS energy_rag\")\n",
    "spark.sql(\"USE energy_rag\")\n",
    "\n",
    "print(\"Database 'energy_rag' created and active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c65363-c75c-47da-b260-7679b2d04eed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Download sample energy documents\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Create directory for PDFs - use /tmp for temporary storage\n",
    "pdf_dir = \"/tmp/energy_docs\"\n",
    "os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "# Sample energy documents (public sources)\n",
    "documents = {\n",
    "    \"aer_directive_017.pdf\": \"https://static.aer.ca/prd/documents/directives/Directive017.pdf\",\n",
    "    \"aer_directive_060.pdf\": \"https://static.aer.ca/prd/documents/directives/Directive060.pdf\"\n",
    "}\n",
    "\n",
    "for filename, url in documents.items():\n",
    "    filepath = os.path.join(pdf_dir, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        response = requests.get(url, timeout=30)\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"  Saved to {filepath}\")\n",
    "    else:\n",
    "        print(f\"  {filename} already exists\")\n",
    "\n",
    "print(f\"\\nTotal documents: {len(os.listdir(pdf_dir))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb3f334-3172-4b8c-8967-5e8582d44196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Extract text from PDFs\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Extract text from all PDFs\n",
    "documents_data = []\n",
    "\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    if filename.endswith('.pdf'):\n",
    "        filepath = os.path.join(pdf_dir, filename)\n",
    "        print(f\"Processing {filename}...\")\n",
    "        \n",
    "        text = extract_text_from_pdf(filepath)\n",
    "        \n",
    "        documents_data.append({\n",
    "            'document_id': filename.replace('.pdf', ''),\n",
    "            'document_name': filename,\n",
    "            'full_text': text,\n",
    "            'char_count': len(text)\n",
    "        })\n",
    "        \n",
    "        print(f\"  Extracted {len(text)} characters\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_docs = pd.DataFrame(documents_data)\n",
    "print(f\"\\nProcessed {len(df_docs)} documents\")\n",
    "print(df_docs[['document_name', 'char_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07226fa-cb4e-4829-ab90-48dee784e8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Chunk documents into smaller pieces\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create chunks for all documents\n",
    "all_chunks = []\n",
    "\n",
    "for idx, row in df_docs.iterrows():\n",
    "    doc_id = row['document_id']\n",
    "    doc_name = row['document_name']\n",
    "    full_text = row['full_text']\n",
    "    \n",
    "    chunks = chunk_text(full_text, chunk_size=1000, overlap=200)\n",
    "    \n",
    "    for chunk_idx, chunk_content in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            'document_id': doc_id,\n",
    "            'document_name': doc_name,\n",
    "            'chunk_id': f\"{doc_id}_chunk_{chunk_idx}\",\n",
    "            'chunk_index': chunk_idx,\n",
    "            'text': chunk_content,\n",
    "            'char_count': len(chunk_content)\n",
    "        })\n",
    "    \n",
    "    print(f\"{doc_name}: {len(chunks)} chunks created\")\n",
    "\n",
    "df_chunks = pd.DataFrame(all_chunks)\n",
    "print(f\"\\nTotal chunks: {len(df_chunks)}\")\n",
    "print(f\"Sample chunk:\\n{df_chunks.iloc[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a2bc677-7c28-405c-9baa-132df704c53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: Generate embeddings for all chunks\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client_canada.embeddings.create(\n",
    "        model=EMBEDDING_DEPLOYMENT,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Generate embeddings in batches\n",
    "embeddings = []\n",
    "batch_size = 50\n",
    "\n",
    "print(f\"Generating embeddings for {len(df_chunks)} chunks...\")\n",
    "print(\"This will take about 3-5 minutes and cost approximately $2-3\")\n",
    "\n",
    "for i in tqdm(range(0, len(df_chunks), batch_size)):\n",
    "    batch = df_chunks.iloc[i:i+batch_size]\n",
    "    \n",
    "    for idx, row in batch.iterrows():\n",
    "        embedding = get_embedding(row['text'])\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    # Small delay to avoid rate limits\n",
    "    time.sleep(0.5)\n",
    "\n",
    "df_chunks['embedding'] = embeddings\n",
    "print(f\"\\nCompleted! Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072cda4c-46b7-470e-bea4-370d82697e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 9: Convert to Spark DataFrame and save to Delta\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df_chunks)\n",
    "\n",
    "# Save as Delta table in Unity Catalog\n",
    "table_name = \"energy_rag.document_chunks\"\n",
    "\n",
    "spark_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"Saved {spark_df.count()} chunks to {table_name}\")\n",
    "\n",
    "# Verify the table\n",
    "result = spark.sql(f\"SELECT COUNT(*) as total FROM {table_name}\").collect()\n",
    "print(f\"Verified: {result[0]['total']} rows in table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1aac92-7ed0-4dc8-a8ce-b6351a980101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM energy_rag.document_chunks;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d43f98-e30e-4501-bedb-d6d1f596133d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 10a: Install Vector Search library\n",
    "%pip install databricks-vectorsearch\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2a339cf-edaf-40a6-b7be-42420de5d621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 10: Create Vector Search Index\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# Initialize Vector Search client\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "# Create vector search endpoint (if not exists)\n",
    "endpoint_name = \"energy_rag_endpoint\"\n",
    "\n",
    "try:\n",
    "    vsc.create_endpoint(name=endpoint_name)\n",
    "    print(f\"Created endpoint: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Endpoint already exists or error: {e}\")\n",
    "\n",
    "# Wait for endpoint to be ready\n",
    "import time\n",
    "time.sleep(30)\n",
    "\n",
    "print(\"Endpoint ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07658b9a-5b94-4cb4-8b11-518ba8de61ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 11b: Check full table name with catalog\n",
    "result = spark.sql(\"DESCRIBE EXTENDED energy_rag.document_chunks\").collect()\n",
    "\n",
    "# Find the catalog info\n",
    "for row in result:\n",
    "    if 'Catalog' in str(row):\n",
    "        print(row)\n",
    "\n",
    "# Also check current catalog\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "print(f\"Current catalog: {current_catalog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19221dc-b438-4d64-9247-7bc75015c741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 11c: Enable Change Data Feed on the table\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE databricks_energy_rag.energy_rag.document_chunks \n",
    "    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Change Data Feed enabled on document_chunks table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2c25fc-e036-4cf1-84c1-dd03872f5e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 11: Create vector search index on the Delta table\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "# Full 3-part names\n",
    "index_name = \"databricks_energy_rag.energy_rag.chunks_vector_index\"\n",
    "source_table = \"databricks_energy_rag.energy_rag.document_chunks\"\n",
    "endpoint_name = \"energy_rag_endpoint\"\n",
    "\n",
    "# Create the index\n",
    "try:\n",
    "    index = vsc.create_delta_sync_index(\n",
    "        endpoint_name=endpoint_name,\n",
    "        index_name=index_name,\n",
    "        source_table_name=source_table,\n",
    "        pipeline_type=\"TRIGGERED\",\n",
    "        primary_key=\"chunk_id\",\n",
    "        embedding_dimension=1536,\n",
    "        embedding_vector_column=\"embedding\"\n",
    "    )\n",
    "    print(f\"Index created: {index_name}\")\n",
    "    print(\"Index is building in background (takes 2-3 minutes)...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c515e8-9442-481f-887a-796cf3a30c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 12: Check index status\n",
    "import time\n",
    "\n",
    "index_name = \"databricks_energy_rag.energy_rag.chunks_vector_index\"\n",
    "\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        index = vsc.get_index(endpoint_name=\"energy_rag_endpoint\", index_name=index_name)\n",
    "        status = index.describe()\n",
    "        print(f\"Status: {status.get('status', {}).get('state', 'BUILDING')}\")\n",
    "        \n",
    "        if status.get('status', {}).get('state') == 'ONLINE':\n",
    "            print(\"Index is ONLINE and ready!\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Checking... {e}\")\n",
    "    \n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddbceb95-f23e-42df-b2ce-f868fcfddf08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 12a: Final status check\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "index = vsc.get_index(endpoint_name=\"energy_rag_endpoint\", \n",
    "                      index_name=\"databricks_energy_rag.energy_rag.chunks_vector_index\")\n",
    "\n",
    "status = index.describe()\n",
    "print(f\"Current status: {status.get('status', {}).get('state', 'UNKNOWN')}\")\n",
    "print(f\"Full status info: {status.get('status', {})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74030c3-a73d-4a49-87fe-4315506bb78a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 13: RAG query function\n",
    "def query_rag_system(question, top_k=5):\n",
    "    \"\"\"\n",
    "    Query the RAG system with a question\n",
    "    Returns relevant context and AI-generated answer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Get embedding for the question\n",
    "    question_embedding = get_embedding(question)\n",
    "    \n",
    "    # Step 2: Search vector index for similar chunks\n",
    "    vsc = VectorSearchClient(disable_notice=True)\n",
    "    index = vsc.get_index(\n",
    "        endpoint_name=\"energy_rag_endpoint\",\n",
    "        index_name=\"databricks_energy_rag.energy_rag.chunks_vector_index\"\n",
    "    )\n",
    "    \n",
    "    results = index.similarity_search(\n",
    "        query_vector=question_embedding,\n",
    "        columns=[\"chunk_id\", \"document_name\", \"text\"],\n",
    "        num_results=top_k\n",
    "    )\n",
    "    \n",
    "    # Step 3: Build context from retrieved chunks\n",
    "    context_chunks = results.get('result', {}).get('data_array', [])\n",
    "    context = \"\\n\\n\".join([chunk[2] for chunk in context_chunks])\n",
    "    \n",
    "    # Step 4: Generate answer using GPT with context\n",
    "    response = client_eastus.chat.completions.create(\n",
    "        model=GPT_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert on Alberta energy regulations. Answer questions based only on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"}\n",
    "        ],\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"sources\": [chunk[1] for chunk in context_chunks],\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "print(\"RAG query function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18a5c0ad-032c-4683-8ca3-7c6f47441b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 13: RAG query function\n",
    "def get_embedding(text):\n",
    "    response = client_canada.embeddings.create(\n",
    "        model=EMBEDDING_DEPLOYMENT,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def query_rag_system(question, top_k=5):\n",
    "    \"\"\"\n",
    "    Query the RAG system with a question\n",
    "    Returns relevant context and AI-generated answer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Get embedding for the question\n",
    "    question_embedding = get_embedding(question)\n",
    "    \n",
    "    # Step 2: Search vector index for similar chunks\n",
    "    vsc = VectorSearchClient(disable_notice=True)\n",
    "    index = vsc.get_index(\n",
    "        endpoint_name=\"energy_rag_endpoint\",\n",
    "        index_name=\"databricks_energy_rag.energy_rag.chunks_vector_index\"\n",
    "    )\n",
    "    \n",
    "    results = index.similarity_search(\n",
    "        query_vector=question_embedding,\n",
    "        columns=[\"chunk_id\", \"document_name\", \"text\"],\n",
    "        num_results=top_k\n",
    "    )\n",
    "    \n",
    "    # Step 3: Build context from retrieved chunks\n",
    "    context_chunks = results.get('result', {}).get('data_array', [])\n",
    "    context = \"\\n\\n\".join([chunk[2] for chunk in context_chunks])\n",
    "    \n",
    "    # Step 4: Generate answer using GPT with context\n",
    "    response = client_eastus.chat.completions.create(\n",
    "        model=GPT_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert on Alberta energy regulations. Answer questions based only on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"}\n",
    "        ],\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"sources\": [chunk[1] for chunk in context_chunks],\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "print(\"RAG query function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8387edc-fcdc-4c07-a330-43f8ade5391e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 14: Test RAG system\n",
    "test_question = \"What are the requirements for flaring in Alberta oil and gas operations?\"\n",
    "\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "print(\"Querying RAG system...\\n\")\n",
    "\n",
    "result = query_rag_system(test_question, top_k=3)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"ANSWER:\\n{result['answer']}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSOURCES: {', '.join(set(result['sources']))}\")\n",
    "print(f\"\\nRetrieved {len(result['sources'])} relevant chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7fd996-8e7c-4064-9bd0-8218627ac6d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 15: Test multiple questions\n",
    "questions = [\n",
    "    \"What are the measurement requirements for oil production?\",\n",
    "    \"What regulations apply to upstream well drilling?\",\n",
    "    \"What is Directive 017 about?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    result = query_rag_system(q, top_k=3)\n",
    "    print(f\"Answer: {result['answer'][:200]}...\")\n",
    "    print(f\"Sources: {set(result['sources'])}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009c5570-abb8-4523-9ccd-a495354cfac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 16: Install Gradio\n",
    "%pip install gradio\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51c31e8-3027-4480-b4df-e8bd1de43d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Cell 17: Simple interactive query interface\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Create widgets\n",
    "question_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Ask a question about Alberta energy regulations...',\n",
    "    description='Question:',\n",
    "    layout=widgets.Layout(width='80%', height='80px')\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description='Get Answer',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_submit(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        question = question_input.value\n",
    "        \n",
    "        if not question.strip():\n",
    "            print(\"Please enter a question.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Question: {question}\\n\")\n",
    "        print(\"Searching documents...\")\n",
    "        \n",
    "        try:\n",
    "            result = query_rag_system(question, top_k=3)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"ANSWER:\")\n",
    "            print(\"=\"*80)\n",
    "            print(result['answer'])\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"SOURCES: {', '.join(set(result['sources']))}\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Display interface\n",
    "display(HTML(\"<h2>Alberta Energy Regulations RAG Assistant</h2>\"))\n",
    "display(HTML(\"<p>Ask questions about AER Directive 017 (Measurement) and Directive 060 (Flaring)</p>\"))\n",
    "display(question_input)\n",
    "display(submit_button)\n",
    "display(output_area)\n",
    "\n",
    "print(\"\\nExample questions:\")\n",
    "print(\"- What are the flaring requirements?\")\n",
    "print(\"- What does Directive 017 cover?\")\n",
    "print(\"- What are measurement requirements for oil production?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53b8ca4c-7ed1-418d-8aea-052cce7872e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 18: Architecture Documentation\n",
    "print(\"\"\"\n",
    "================================================================================\n",
    "ALBERTA ENERGY RAG SYSTEM - ARCHITECTURE\n",
    "================================================================================\n",
    "\n",
    "COMPONENTS:\n",
    "-----------\n",
    "1. Data Ingestion\n",
    "   - Source: Alberta Energy Regulator (AER) public PDFs\n",
    "   - Documents: Directive 017 (Measurement), Directive 060 (Flaring)\n",
    "   - Storage: /tmp on Databricks cluster\n",
    "\n",
    "2. Document Processing\n",
    "   - PDF text extraction: PyPDF2\n",
    "   - Chunking: 1000 chars with 200 char overlap\n",
    "   - Total chunks: 1,396\n",
    "\n",
    "3. Embedding Generation\n",
    "   - Model: Azure OpenAI text-embedding-3-small\n",
    "   - Dimensions: 1536\n",
    "   - Location: Canada East region\n",
    "   - Cost: ~$2 for 1.4M tokens\n",
    "\n",
    "4. Vector Storage & Search\n",
    "   - Storage: Unity Catalog Delta table\n",
    "   - Table: databricks_energy_rag.energy_rag.document_chunks\n",
    "   - Vector Index: Databricks Vector Search (HNSW algorithm)\n",
    "   - Endpoint: energy_rag_endpoint\n",
    "   - Index sync: Delta Change Data Feed enabled\n",
    "\n",
    "5. Query & Generation\n",
    "   - Query embedding: Same as step 3\n",
    "   - Similarity search: Top-K retrieval (default K=3)\n",
    "   - LLM: Azure OpenAI gpt-4o-mini\n",
    "   - Location: East US 2 region\n",
    "   - Context window: Retrieved chunks + question\n",
    "\n",
    "DATA FLOW:\n",
    "----------\n",
    "User Question \n",
    "  → Embed question (Azure OpenAI Canada East)\n",
    "  → Search vector index (Databricks Vector Search)\n",
    "  → Retrieve top-K similar chunks\n",
    "  → Build context from chunks\n",
    "  → Generate answer (Azure OpenAI East US 2 + context)\n",
    "  → Return answer + sources\n",
    "\n",
    "KEY FEATURES:\n",
    "-------------\n",
    "- Unified lakehouse: No separate vector database needed\n",
    "- Governance: Unity Catalog handles permissions & lineage\n",
    "- Cost efficient: Delta storage + triggered index updates\n",
    "- Scalable: Can handle millions of documents\n",
    "- Explainable: Always cites source documents\n",
    "\n",
    "DEPLOYMENT:\n",
    "-----------\n",
    "- Cluster: Single-node Standard_DS3_v2 (auto-terminate 30min)\n",
    "- Runtime: Databricks 15.4 LTS\n",
    "- Compute cost: ~$0.55/DBU hour + VM cost\n",
    "\n",
    "\n",
    "================================================================================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8065fe7-493c-4e37-af06-d13b79eed677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 19: Generate README content for GitHub\n",
    "readme_content = \"\"\"\n",
    "# Alberta Energy Regulations RAG Assistant\n",
    "\n",
    "AI-powered question-answering system for Alberta Energy Regulator directives using Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "## Business Problem\n",
    "Energy companies and regulatory professionals spend hours searching through dense regulatory documents. This RAG system provides instant, accurate answers grounded in official AER directives.\n",
    "\n",
    "## Tech Stack\n",
    "- **Azure Databricks**: Unity Catalog, Delta Lake, Vector Search\n",
    "- **Azure OpenAI**: text-embedding-3-small, gpt-4o-mini\n",
    "- **Data Source**: Alberta Energy Regulator public directives\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "User Question → Embedding → Vector Search → Context Retrieval → LLM Generation → Answer\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "1. **Unified Lakehouse**: Single Delta table stores text + embeddings + metadata\n",
    "2. **Databricks Vector Search**: No separate vector database needed\n",
    "3. **Source Attribution**: Every answer cites specific documents\n",
    "4. **Cost Efficient**: ~$15 total development cost, scales to production\n",
    "\n",
    "## Documents Indexed\n",
    "- AER Directive 017: Measurement Requirements (866K chars)\n",
    "- AER Directive 060: Upstream Petroleum Industry Flaring (250K chars)\n",
    "- Total: 1,396 chunks with 1536-dimensional embeddings\n",
    "\n",
    "## Demo Queries\n",
    "- \"What are the flaring requirements in Alberta?\"\n",
    "- \"What does Directive 017 cover?\"\n",
    "- \"What are measurement requirements for oil production?\"\n",
    "\n",
    "## Technical Highlights\n",
    "- **Delta Lake**: ACID transactions, time travel, Change Data Feed\n",
    "- **Unity Catalog**: Centralized governance and lineage tracking\n",
    "- **Vector Search**: HNSW index for sub-second similarity search\n",
    "- **Multi-region**: Optimized endpoints (Canada East + East US 2)\n",
    "\n",
    "\n",
    "## Metrics\n",
    "- Query latency: 2-3 seconds end-to-end\n",
    "- Accuracy: Answers grounded in source text\n",
    "- Scalability: Tested with 1.4K chunks, scales to millions\n",
    "\n",
    "## Future Enhancements\n",
    "- Add more AER directives and regulations\n",
    "- Multi-modal: Include diagrams and tables from PDFs\n",
    "- Real-time updates: Auto-sync when new directives published\n",
    "- Fine-tuned embeddings for domain-specific terminology\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save to file for easy copy-paste\n",
    "with open(\"/tmp/README.md\", \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"README.md content created at /tmp/README.md\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(readme_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c9ffeac-dba7-4a89-afac-f15444082cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5149214519060675,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_energy_rag_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
